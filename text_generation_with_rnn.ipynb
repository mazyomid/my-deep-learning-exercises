{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "nCpmNpYqHaKU"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import os\n",
        "import time"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eRAoySL1cBAz"
      },
      "source": [
        "# download the dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8ryYajsUJCWi",
        "outputId": "7eba8ac8-c906-4761-ce43-e29cbfd8a0da"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt\n",
            "1122304/1115394 [==============================] - 0s 0us/step\n",
            "1130496/1115394 [==============================] - 0s 0us/step\n"
          ]
        }
      ],
      "source": [
        "path_to_file = tf.keras.utils.get_file('shakespeare.txt', 'https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pUa46Y8sJ2rP",
        "outputId": "f6b199cb-836a-4057-e20f-129d8a4a4d6c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Length of text: 1115394 characters\n"
          ]
        }
      ],
      "source": [
        "text = open(path_to_file, 'rb').read().decode(encoding='utf-8')\n",
        "print(f'Length of text: {len(text)} characters')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_mtO76CnO6Hg",
        "outputId": "4096b338-8622-403e-fb08-f04346799085"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "First Citizen:\n",
            "Before we proceed any further, hear me speak.\n",
            "\n",
            "All:\n",
            "Speak, speak.\n",
            "\n",
            "First Citizen:\n",
            "You are all resolved rather to die than to famish?\n",
            "\n",
            "All:\n",
            "Resolved. resolved.\n",
            "\n",
            "First Citizen:\n",
            "First, you know Caius Marcius is chief enemy to the people.\n",
            "\n"
          ]
        }
      ],
      "source": [
        "print(text[:250])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0Zr6ZJzFWd1E"
      },
      "source": [
        "# Process the text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HV_E2x4MPF92",
        "outputId": "eb9bfd30-bb23-41ae-e29a-f95d6a238659"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "65 unique characters\n"
          ]
        }
      ],
      "source": [
        "vocab = sorted(set(text))\n",
        "print(f'{len(vocab)} unique characters')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "eBItPwL_RSF1"
      },
      "outputs": [],
      "source": [
        "# The tf.keras.layers.StringLookup layer can convert each character into a numeric ID.\n",
        "# It just needs the text to be split into tokens first.\n",
        "ids_from_chars = tf.keras.layers.StringLookup(\n",
        "    vocabulary=list(vocab), mask_token=None)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "YrcGCr9eR-yz"
      },
      "outputs": [],
      "source": [
        "# Here instead of passing the original vocabulary \n",
        "#generated with sorted(set(text)) use the get_vocabulary() method \n",
        "# of the tf.keras.layers.StringLookup layer so that the [UNK] tokens is set the same way.\n",
        "chars_from_ids = tf.keras.layers.StringLookup(\n",
        "    vocabulary=ids_from_chars.get_vocabulary(), invert=True, mask_token=None)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "xuBz7xODVnfn"
      },
      "outputs": [],
      "source": [
        "#join the characters back into strings\n",
        "def text_from_ids(ids):\n",
        "  return tf.strings.reduce_join(chars_from_ids(ids), axis=-1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BNBx6M6ZWWXe",
        "outputId": "5a5385c0-b559-4d35-a63e-55ee841eee7c"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(1115394,), dtype=int64, numpy=array([19, 48, 57, ..., 46,  9,  1])>"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ],
      "source": [
        "all_ids = ids_from_chars(tf.strings.unicode_split(text, 'UTF-8'))\n",
        "all_ids"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "OQZD5-nyXf0l"
      },
      "outputs": [],
      "source": [
        "ids_dataset = tf.data.Dataset.from_tensor_slices(all_ids)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mXXppTaRYFBE",
        "outputId": "55795ba1-14ea-48f3-ecf9-abb2fe792907"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "F\n",
            "i\n",
            "r\n",
            "s\n",
            "t\n",
            " \n",
            "C\n",
            "i\n",
            "t\n",
            "i\n"
          ]
        }
      ],
      "source": [
        "for ids in ids_dataset.take(10):\n",
        "    print(chars_from_ids(ids).numpy().decode('utf-8'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "BH0lAj0rcQlU"
      },
      "outputs": [],
      "source": [
        "seq_length = 100\n",
        "examples_per_epoch = len(text)//(seq_length+1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hwNhT6medFNW",
        "outputId": "6fca7f04-d021-474e-d3a7-a2e60de6b76a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tf.Tensor(\n",
            "[b'F' b'i' b'r' b's' b't' b' ' b'C' b'i' b't' b'i' b'z' b'e' b'n' b':'\n",
            " b'\\n' b'B' b'e' b'f' b'o' b'r' b'e' b' ' b'w' b'e' b' ' b'p' b'r' b'o'\n",
            " b'c' b'e' b'e' b'd' b' ' b'a' b'n' b'y' b' ' b'f' b'u' b'r' b't' b'h'\n",
            " b'e' b'r' b',' b' ' b'h' b'e' b'a' b'r' b' ' b'm' b'e' b' ' b's' b'p'\n",
            " b'e' b'a' b'k' b'.' b'\\n' b'\\n' b'A' b'l' b'l' b':' b'\\n' b'S' b'p' b'e'\n",
            " b'a' b'k' b',' b' ' b's' b'p' b'e' b'a' b'k' b'.' b'\\n' b'\\n' b'F' b'i'\n",
            " b'r' b's' b't' b' ' b'C' b'i' b't' b'i' b'z' b'e' b'n' b':' b'\\n' b'Y'\n",
            " b'o' b'u' b' '], shape=(101,), dtype=string)\n"
          ]
        }
      ],
      "source": [
        "sequences = ids_dataset.batch(seq_length+1, drop_remainder=True)\n",
        "\n",
        "for seq in sequences.take(1):\n",
        "  print(chars_from_ids(seq))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "niF_wohadMJJ",
        "outputId": "df283e8b-fac8-483e-d89b-421cae20db7e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "b'First Citizen:\\nBefore we proceed any further, hear me speak.\\n\\nAll:\\nSpeak, speak.\\n\\nFirst Citizen:\\nYou '\n",
            "b'are all resolved rather to die than to famish?\\n\\nAll:\\nResolved. resolved.\\n\\nFirst Citizen:\\nFirst, you k'\n",
            "b\"now Caius Marcius is chief enemy to the people.\\n\\nAll:\\nWe know't, we know't.\\n\\nFirst Citizen:\\nLet us ki\"\n",
            "b\"ll him, and we'll have corn at our own price.\\nIs't a verdict?\\n\\nAll:\\nNo more talking on't; let it be d\"\n",
            "b'one: away, away!\\n\\nSecond Citizen:\\nOne word, good citizens.\\n\\nFirst Citizen:\\nWe are accounted poor citi'\n"
          ]
        }
      ],
      "source": [
        "for seq in sequences.take(5):\n",
        "  print(text_from_ids(seq).numpy())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "SLMHf7-odapB"
      },
      "outputs": [],
      "source": [
        "def split_input_target(sequence):\n",
        "    input_text = sequence[:-1]\n",
        "    target_text = sequence[1:]\n",
        "    return input_text, target_text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "uSbEkroxeLjZ"
      },
      "outputs": [],
      "source": [
        "dataset = sequences.map(split_input_target)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PRD1p-LVeSqg",
        "outputId": "5bf52c60-e42f-43b0-d3cf-d91caa6b9030"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input : b'First Citizen:\\nBefore we proceed any further, hear me speak.\\n\\nAll:\\nSpeak, speak.\\n\\nFirst Citizen:\\nYou'\n",
            "Target: b'irst Citizen:\\nBefore we proceed any further, hear me speak.\\n\\nAll:\\nSpeak, speak.\\n\\nFirst Citizen:\\nYou '\n"
          ]
        }
      ],
      "source": [
        "for input_example, target_example in dataset.take(1):\n",
        "    print(\"Input :\", text_from_ids(input_example).numpy())\n",
        "    print(\"Target:\", text_from_ids(target_example).numpy())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qNhydX63eWbx",
        "outputId": "2b6954e3-a956-4146-f2aa-fd6c31d5c745"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<PrefetchDataset element_spec=(TensorSpec(shape=(64, 100), dtype=tf.int64, name=None), TensorSpec(shape=(64, 100), dtype=tf.int64, name=None))>"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ],
      "source": [
        "BATCH_SIZE = 64\n",
        "BUFFER_SIZE = 10000\n",
        "\n",
        "dataset = (\n",
        "    dataset\n",
        "    .shuffle(BUFFER_SIZE)\n",
        "    .batch(BATCH_SIZE, drop_remainder=True)\n",
        "    .prefetch(tf.data.experimental.AUTOTUNE))\n",
        "\n",
        "dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TiHpyWTRgNNx"
      },
      "source": [
        "# create model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "NQ_tLEBXgJar"
      },
      "outputs": [],
      "source": [
        "# Length of the vocabulary in chars\n",
        "vocab_size = len(vocab)\n",
        "\n",
        "# The embedding dimension \n",
        "embedding_dim = 256\n",
        "\n",
        "# Number of RNN units\n",
        "rnn_units = 1024"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class MyModel(tf.keras.Model):\n",
        "  def __init__(self, vocab_size, embedding_dim, rnn_units):\n",
        "    super().__init__(self)\n",
        "    self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
        "    self.gru = tf.keras.layers.GRU(rnn_units,\n",
        "                                   return_sequences=True,\n",
        "                                   return_state=True)\n",
        "    self.dense = tf.keras.layers.Dense(vocab_size)\n",
        "\n",
        "  def call(self, inputs, states=None, return_state=False, training=False):\n",
        "    x = inputs\n",
        "    x = self.embedding(x, training=training)\n",
        "    if states is None:\n",
        "      states = self.gru.get_initial_state(x)\n",
        "    x, states = self.gru(x, initial_state=states, training=training)\n",
        "    x = self.dense(x, training=training)\n",
        "\n",
        "    if return_state:\n",
        "      return x, states\n",
        "    else:\n",
        "      return x"
      ],
      "metadata": {
        "id": "MxFk-ZjpHEHH"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = MyModel(\n",
        "    # Be sure the vocabulary size matches the `StringLookup` layers.\n",
        "    vocab_size=len(ids_from_chars.get_vocabulary()),\n",
        "    embedding_dim=embedding_dim,\n",
        "    rnn_units=rnn_units)"
      ],
      "metadata": {
        "id": "fu32eV5oHIcA"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p0XHPEIhgDMq",
        "outputId": "21bcb689-6491-4dda-f1a6-0f1a9799275f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(64, 100, 66) # (batch_size, sequence_length, vocab_size)\n"
          ]
        }
      ],
      "source": [
        "for input_example_batch, target_example_batch in dataset.take(1):\n",
        "    example_batch_predictions = model(input_example_batch)\n",
        "    print(example_batch_predictions.shape, \"# (batch_size, sequence_length, vocab_size)\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2Xy--lXLSdaX",
        "outputId": "41d5a584-f38f-44b1-eec4-b9b2cbfee6aa"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"my_model\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding (Embedding)       multiple                  16896     \n",
            "                                                                 \n",
            " gru (GRU)                   multiple                  3938304   \n",
            "                                                                 \n",
            " dense (Dense)               multiple                  67650     \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 4,022,850\n",
            "Trainable params: 4,022,850\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "oYSeCKsFgsIf"
      },
      "outputs": [],
      "source": [
        "# Directory where the checkpoints will be saved\n",
        "checkpoint_dir = './training_checkpoints'\n",
        "# Name of the checkpoint files\n",
        "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt_{epoch}\")\n",
        "\n",
        "checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
        "    filepath=checkpoint_prefix,\n",
        "    save_weights_only=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "D-uMzVZMj3Po"
      },
      "outputs": [],
      "source": [
        "EPOCHS = 5"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "8YpDPu1KlH6L"
      },
      "outputs": [],
      "source": [
        "loss = tf.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
        "model.compile(optimizer='adam',loss = loss)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "BuO6ivAUkul-"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7gIiHRcmj2j0",
        "outputId": "4fc83d17-61ea-4aee-f79c-a108f62ceb9c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "172/172 [==============================] - 35s 181ms/step - loss: 2.7487\n",
            "Epoch 2/5\n",
            "172/172 [==============================] - 33s 182ms/step - loss: 2.0035\n",
            "Epoch 3/5\n",
            "172/172 [==============================] - 33s 182ms/step - loss: 1.7213\n",
            "Epoch 4/5\n",
            "172/172 [==============================] - 36s 188ms/step - loss: 1.5575\n",
            "Epoch 5/5\n",
            "172/172 [==============================] - 34s 182ms/step - loss: 1.4564\n"
          ]
        }
      ],
      "source": [
        "history = model.fit(dataset, epochs=EPOCHS, callbacks=[checkpoint_callback])"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "class OneStep(tf.keras.Model):\n",
        "  def __init__(self, model, chars_from_ids, ids_from_chars, temperature=1.0):\n",
        "    super().__init__()\n",
        "    self.temperature = temperature\n",
        "    self.model = model\n",
        "    self.chars_from_ids = chars_from_ids\n",
        "    self.ids_from_chars = ids_from_chars\n",
        "\n",
        "    # Create a mask to prevent \"[UNK]\" from being generated.\n",
        "    skip_ids = self.ids_from_chars(['[UNK]'])[:, None]\n",
        "    sparse_mask = tf.SparseTensor(\n",
        "        # Put a -inf at each bad index.\n",
        "        values=[-float('inf')]*len(skip_ids),\n",
        "        indices=skip_ids,\n",
        "        # Match the shape to the vocabulary\n",
        "        dense_shape=[len(ids_from_chars.get_vocabulary())])\n",
        "    self.prediction_mask = tf.sparse.to_dense(sparse_mask)\n",
        "\n",
        "  @tf.function\n",
        "  def generate_one_step(self, inputs, states=None):\n",
        "    # Convert strings to token IDs.\n",
        "    input_chars = tf.strings.unicode_split(inputs, 'UTF-8')\n",
        "    input_ids = self.ids_from_chars(input_chars).to_tensor()\n",
        "\n",
        "    # Run the model.\n",
        "    # predicted_logits.shape is [batch, char, next_char_logits]\n",
        "    predicted_logits, states = self.model(inputs=input_ids, states=states,\n",
        "                                          return_state=True)\n",
        "    # Only use the last prediction.\n",
        "    predicted_logits = predicted_logits[:, -1, :]\n",
        "    predicted_logits = predicted_logits/self.temperature\n",
        "    # Apply the prediction mask: prevent \"[UNK]\" from being generated.\n",
        "    predicted_logits = predicted_logits + self.prediction_mask\n",
        "\n",
        "    # Sample the output logits to generate token IDs.\n",
        "    predicted_ids = tf.random.categorical(predicted_logits, num_samples=1)\n",
        "    predicted_ids = tf.squeeze(predicted_ids, axis=-1)\n",
        "\n",
        "    # Convert from token ids to characters\n",
        "    predicted_chars = self.chars_from_ids(predicted_ids)\n",
        "\n",
        "    # Return the characters and model state.\n",
        "    return predicted_chars, states"
      ],
      "metadata": {
        "id": "tbAtxo2EBiXf"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "one_step_model = OneStep(model, chars_from_ids, ids_from_chars)"
      ],
      "metadata": {
        "id": "q0tv2BW2Fh6p"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "jsIEYd-pmELZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e08a94ec-4419-4d38-dd7e-c3eb1df7a54d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ROMEO:\n",
            "He looks is fortant; sound, most thou allust\n",
            "I pardon of the elier.\n",
            "Groboty: 'tis heaven! O'erched; I know thy bory,\n",
            "To proud a wented desinged these\n",
            "werouble speak, orwerd, were has not coursed.\n",
            "\n",
            "ANGELO:\n",
            "Fie, as I must hea? Thou storts hair your isfer,\n",
            "To go to be tongue! where's no soon,\n",
            "As well to most usurped of the pistory.\n",
            "And so be do; I do be no man!\n",
            "What! are not, well make you sweet is friends you?\n",
            "\n",
            "Clifford!\n",
            "\n",
            "Clown:\n",
            "His wratch were man roy provoss yourself intelling father?\n",
            "Do Jove at much printed for my steal,\n",
            "Thou hast crown, exectuness a foon, good men,\n",
            "I bittee is to Licet: to stat temmend\n",
            "'smad. O, beseace with me; I live, it Henry,\n",
            "By aggear, common pleased! There sty father te die,\n",
            "And where present poor met abready house thess\n",
            "Have suil'd the closes is outhen in thie foll of to use\n",
            "to parnon that is nears, for how the fair sweets\n",
            "Being 'sull arms a day brave my father's lobe.\n",
            "\n",
            "ROMEO:\n",
            "Heaven were well three piercy, but being so het the greater\n",
            "Ent us to players out t \n",
            "\n",
            "________________________________________________________________________________\n",
            "\n",
            "Run time: 4.9591064453125\n"
          ]
        }
      ],
      "source": [
        "import time\n",
        "start = time.time()\n",
        "states = None\n",
        "next_char = tf.constant(['ROMEO:'])\n",
        "result = [next_char]\n",
        "\n",
        "for n in range(1000):\n",
        "  next_char, states = one_step_model.generate_one_step(next_char, states=states)\n",
        "  result.append(next_char)\n",
        "\n",
        "result = tf.strings.join(result)\n",
        "end = time.time()\n",
        "print(result[0].numpy().decode('utf-8'), '\\n\\n' + '_'*80)\n",
        "print('\\nRun time:', end - start)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "start = time.time()\n",
        "states = None\n",
        "next_char = tf.constant(['ROMEO:', 'ROMEO:', 'ROMEO:', 'ROMEO:', 'ROMEO:'])\n",
        "result = [next_char]\n",
        "\n",
        "for n in range(1000):\n",
        "  next_char, states = one_step_model.generate_one_step(next_char, states=states)\n",
        "  result.append(next_char)\n",
        "\n",
        "result = tf.strings.join(result)\n",
        "end = time.time()\n",
        "print(result, '\\n\\n' + '_'*80)\n",
        "print('\\nRun time:', end - start)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-8nJonGbTbLI",
        "outputId": "9fe599e8-1d8a-40dc-b4ed-594bc52550cd"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tf.Tensor(\n",
            "[b\"ROMEO:\\nBoliad, why doom that report every dadners;\\nAnd yet alone my Lord, I'll we arey\\nmonest, sments, Bytand, my love is virtue him,\\nWhich cross not pety out, rascels, if thou mad us\\nthe beggars that weep'd my heart!\\nThe oution a'e, stand I tell your most.\\nA matcely, to our queen to my elders,\\nTo place he har shall pressied lights bow,\\nAnd man, what, my too canst she?\\nThou hast that the sworal common where at liet.\\n\\nDUKE OF AUMERLE:\\n'Tis a knowled are for, they shall weppering loss\\nmy strange, Camillo will see:\\nTo lest your missabe us it?\\n\\nRONTO:\\nOr if you most:\\nAf it scarved upon the greates\\nThan heaven dream and badest no more;\\nAs honour.\\n\\nFirst Seenat I being eye,\\nOn with us he is it ta-know her, to our husband!\\nAnd most good married, both, as now\\nI slew mine away.\\n\\nSecond Mine:\\nWhithless woild the duke.\\n\\nISABELLA:\\nI Gams no printened commons!\\nTo should ble more creadunes?\\nFarewell, give me; my lords, so doing so holy!\\n\\nKING HENRY BI\\nCase your worls clain,\\nAnd, what this york so bosomiti\"\n",
            " b\"ROMEO:\\nBut never bless of the duaken of Geep-bedent?\\nWho! hollow'st thou! say is your fassion,\\nI'll kind a king, Andul Beators go\\nrobost the warwick is of the best amort.\\n\\nLUCIO:\\nTherefore! a heaven's joys,\\nYou mean to be hour resolve to your stime to pardon' dose, to bes\\nWotherd and stool my life for the world\\nSo lrave all with unksomain's lond day, give should bo nuble nembered.\\n\\nHENRY BOLINGBROKE:\\nThe ranget and two you?\\n\\nServant:\\nYou have upful my jount! call receedes to BonCIOH:\\nCome, who know'st she long word of all us.\\nThey have gone upon him. The roys to do\\nAs myself stor is no tramble to proporte comp.\\nFor this compan'd respects of her lipels,\\nAs if the murtersor but, and sign'd to you.\\n\\nANGELO:\\nO, calley, like and woman it.\\n\\nGLOUCESTER:\\n\\nISABELLA:\\nThen, ally moon'd upon this foth:\\nO' thrine is to first indeed, Buldita-moultea\\nBut soucation of a content of:\\nNomet meaner you shall bean the parour, though the ear mound\\nIs present pursy, and I treal,\\nI have for the issubutannels; but I\"\n",
            " b\"ROMEO:\\nBalled blood, and as I must to ber,\\nThis we do restict at the\\ncondoties, a boy, for the poorsemish keep me.\\nThere rone this close you, to offer the bot\\nWill endoous my monest her, and\\na poor fow Ovendry:\\nAnd the keep against this love; what triblius buh\\nthe trubhed of the Roman, we'll so.\\n\\nDUKE VINCENTIO:\\nTo wilon, would all my worship is not\\nWy are never hath of\\nWith the first prove to bed son.\\n\\nCOMINAUS:\\nO. he would not poor is,\\nAnd vare no presence; my lord.\\n\\nQUEEN ELIZABETH:\\nMay, there, doof maik, 'til the world mare she shool\\nThat savess his drum son; thus it such not\\nWhen never bear to a sequen; and that thou are\\nthe dointy inkind so joised saintain:\\nNo not unoverer more.\\nComes look, the dume pertance of his\\nmastird of GRUSo\\nthis lodd with his ents the voil.\\nThus for I am,\\nleave not than the sprink are speen,\\nWe pass'dus many--\\nWithout your son, who vasties spoil.\\n\\nCAMISLON:\\nHe's no bear, in the old bannow: Butcy forsting avoity minis being\\nAumerer my patred cornelly:\\nHad camals\"\n",
            " b\"ROMEO:\\nWelcome, to was my life of blood;\\nWhy baye comes you alone,\\nWho she shall never by that, tair of thy\\ncourt of While,\\nAunthout and thou could\\nA married peaces: and dawn be as love, dit fortid them,\\nThy protence of thouson to love to prison.\\nLet this dead passis do; for I saw it from me, both!\\n\\nNurse:\\nMy lord! I will pain to be urt.\\n\\nSEBRUSY:\\nI kill then, sir; how shall I princely to me: KING HENRY VI:\\nLessent, While I double kinger:\\nCastle, going in gently by the good day, thou art\\nOf it in oursesce you. which is a utuson; where way some over\\nOnfected to the king, and many greatest.\\n\\nServant:\\nClyed to die; it good now, by no punisf,\\nI do not susp the dear to upon thee,\\nThis stay he dues told me might for a doublew as is.\\n\\nVOLSENIUS:\\nThough she homes!\\nMy band of God alazed,\\nYou canly I begry, my mole in a confersion:\\nFor the air so dright ham toward, Kenaffer throw the king.\\n\\nANGELO:\\nMy highness are feeling, I beside.\\nNom, 'Bently JoRIS:\\nI do posup me to make them now,\\nFor that she take\"\n",
            " b\"ROMEO:\\nI'll pity 'tis for eljes.\\n\\nHENRY PHIRBE:\\nBe tainter, matam: you took me, quield! On God!\\nLook, this is thom at changed, devis;\\nO great Parcasones, pidy, sir; we brother.\\n\\nESCALUS:\\nFor on; I spork is this?\\n\\nGLEY:\\nMadam, I,\\nWho bried my despect remip?\\nUncelfire to see it one could bound o'e woo from their famour not oll togething me\\nAs well. I must too make it,\\nThat eve, women, a termity we douth the bring.\\nAnd this consents are man's free complain;\\nI may marriane of our knowledge; too have\\nAny she do no give for the birlair.\\n\\nCOMINIUS:\\nHast man!\\n\\nFRIAR LAURENCE:\\nComf the is from to cross again,---poses?\\n\\nfirst Servingman: Of I\\nhave thempend upon thy kind and offring:\\nThe but they love the hal, not unbrok which should come, being fol\\neymedies to grace a streatidy dived\\nTo speak conient, proRonties.\\n\\nHORTENSIO:\\nOne wor now. See this livery person father.\\n\\nBessere were for the corour.\\n\\nCOMINIUS:\\nHow loves, wish thy agree and the papsal,\\nA over of his brother in-citizent.\\n\\nBAPTISTA:\\nNones,\"], shape=(5,), dtype=string) \n",
            "\n",
            "________________________________________________________________________________\n",
            "\n",
            "Run time: 5.620705604553223\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tf.saved_model.save(one_step_model, 'one_step')\n",
        "one_step_reloaded = tf.saved_model.load('one_step')"
      ],
      "metadata": {
        "id": "5jKCdCQ9BC_g"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "XS8txbaZTk6h"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "text generation with rnn.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}